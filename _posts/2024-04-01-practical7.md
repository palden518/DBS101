---
title: Practical 7 - Database Storage Structures
sidebar:
  nav: pract-en
aside:
  toc: true
---
# Guided Session

Tasks on Storage and buffer management - Simulate disk blocks, RAID configurations, and buffer pools.

## Task 1: Disk Block Implementation
This implementation simulates a disk with a fixed number of blocks and a fixed block size. The allocate method allocates contiguous blocks for a given data, and the read, write, and deallocate methods perform the respective operations on the allocated blocks. The defragment method consolidates the allocated blocks to eliminate fragmentation

```javascript
import math

class DiskBlock:
    def __init__(self, block_size, num_blocks):
        self.block_size = block_size
        self.num_blocks = num_blocks
        self.blocks = [b'\0' * block_size for _ in range(num_blocks)]
        self.free_blocks = list(range(num_blocks))
        self.allocated_blocks = {}

    def allocate(self, data):
        required_blocks = math.ceil(len(data) / self.block_size)
        allocated_blocks = []

        if len(self.free_blocks) >= required_blocks:
            for _ in range(required_blocks):
                block_idx = self.free_blocks.pop(0)
                allocated_blocks.append(block_idx)
                self.allocated_blocks[block_idx] = data[:self.block_size]
                data = data[self.block_size:]

            return allocated_blocks

        return None

    def read(self, block_indices):
        data = b''
        for idx in block_indices:
            data += self.blocks[idx]
        return data

    def write(self, block_indices, data):
        for idx, block_data in zip(block_indices, [data[i:i+self.block_size] for i in range(0, len(data), self.block_size)]):
            self.blocks[idx] = block_data

    def deallocate(self, block_indices):
        for idx in block_indices:
            if idx in self.allocated_blocks:
                self.blocks[idx] = b'\0' * self.block_size
                self.free_blocks.append(idx)
                del self.allocated_blocks[idx]

    def defragment(self):
        new_blocks = [b'\0' * self.block_size for _ in range(self.num_blocks)]
        new_free_blocks = list(range(self.num_blocks))
        new_allocated_blocks = {}

        for blocks, data in self.allocated_blocks.items():
            required_blocks = math.ceil(len(data) / self.block_size)
            allocated_blocks = new_free_blocks[:required_blocks]
            new_free_blocks = new_free_blocks[required_blocks:]

            for idx, block_data in zip(allocated_blocks, [data[i:i+self.block_size] for i in range(0, len(data), self.block_size)]):
                new_blocks[idx] = block_data
                new_allocated_blocks[idx] = block_data

        self.blocks = new_blocks
        self.free_blocks = new_free_blocks
        self.allocated_blocks = new_allocated_blocks


# Create a disk with 10 blocks of size 512 bytes
disk = DiskBlock(block_size=512, num_blocks=10)

# Allocate space for data
data1 = b'Hello, World!' * 100
allocated_blocks1 = disk.allocate(data1)
print(f"Allocated blocks for data1: {allocated_blocks1}")
# Allocated blocks for data1: [0, 1]

# Read data
read_data1 = disk.read(allocated_blocks1)
print(f"Read data1: {read_data1}")
# Read data1: b'Hello, World!Hello, World!Hello, World!...'

# Write new data
data2 = b'Goodbye!' * 200
disk.write(allocated_blocks1, data2)
read_data2 = disk.read(allocated_blocks1)
print(f"Read data2: {read_data2}")
# Read data2: b'Goodbye!Goodbye!Goodbye!Goodbye!...'

# Deallocate blocks
disk.deallocate(allocated_blocks1)

# Allocate space for new data after deallocation
data3 = b'Hello, Again!' * 50
allocated_blocks3 = disk.allocate(data3)
print(f"Allocated blocks for data3: {allocated_blocks3}")
# Allocated blocks for data3: [0, 1]

# Defragment the disk
disk.defragment()
read_data3 = disk.read(allocated_blocks3)
print(f"Read data3 after defragmentation: {read_data3}")
```

In the above implementation:
- We create a DiskBlock instance with 10 blocks of size 512 bytes.
- We allocate contiguous blocks for data1 and read/write data to those blocks.
- After deallocating data1, we allocate new blocks for data3.
- Finally, we defragment the disk, which consolidates the allocated blocks for data3.

## Task 2: 

This implementation simulates different RAID levels (0, 1, and 5) using a fixed block size and a configurable number of disks. The read and write methods handle data striping, mirroring, and parity calculations based on the chosen RAID level. The rebuild method simulates the process of rebuilding data on a failed disk using spare disks.

```javascript
import math

class DiskBlock:
    def __init__(self, block_size):
        self.block_size = block_size
        self.data = b'\0' * block_size

    def read(self):
        return self.data

    def write(self, data):
        self.data = data

class RAIDArray:
    def __init__(self, block_size, num_disks, raid_level, num_spare=0):
        self.block_size = block_size
        self.num_disks = num_disks
        self.raid_level = raid_level
        self.disks = [DiskBlock(block_size) for _ in range(num_disks + num_spare)]
        self.spare_disks = self.disks[-num_spare:]
        self.data_disks = self.disks[:-num_spare]

    def read(self, block_idx):
        if self.raid_level == 0:
            return b''.join(disk.read() for disk in self.data_disks[block_idx::self.num_disks])
        elif self.raid_level == 1:
            return self.data_disks[block_idx // 2].read()
        elif self.raid_level == 5:
            data = []
            parity_idx = block_idx % (self.num_disks - 1)
            for i in range(self.num_disks - 1):
                if i != parity_idx:
                    data.append(self.data_disks[i].read())
            parity = self.data_disks[parity_idx].read()
            return b''.join(data)

    def write(self, block_idx, data):
        if self.raid_level == 0:
            for i, disk in enumerate(self.data_disks[block_idx::self.num_disks]):
                disk.write(data[i*self.block_size:(i+1)*self.block_size])
        elif self.raid_level == 1:
            self.data_disks[block_idx // 2].write(data)
            self.data_disks[(block_idx // 2) + (self.num_disks // 2)].write(data)
        elif self.raid_level == 5:
            parity_idx = block_idx % (self.num_disks - 1)
            parity = b'\0' * self.block_size
            for i in range(self.num_disks - 1):
                if i != parity_idx:
                    self.data_disks[i].write(data[i*self.block_size:(i+1)*self.block_size])
                    parity = bytes(a ^ b for a, b in zip(parity, data[i*self.block_size:(i+1)*self.block_size]))
            self.data_disks[parity_idx].write(parity)

    def rebuild(self, failed_disk_idx):
        if self.raid_level == 1:
            self.data_disks[failed_disk_idx] = self.spare_disks.pop()
            for i in range(self.num_disks // 2):
                self.data_disks[failed_disk_idx].write(self.data_disks[i].read())
        elif self.raid_level == 5:
            self.data_disks[failed_disk_idx] = self.spare_disks.pop()
            for block_idx in range(len(self.data_disks[0].data) // self.block_size):
                parity_idx = block_idx % (self.num_disks - 1)
                if parity_idx == failed_disk_idx:
                    data = []
                    for i in range(self.num_disks - 1):
                        if i != failed_disk_idx:
                            data.append(self.data_disks[i].read())
                    parity = bytes(reduce(lambda a, b: bytes(c ^ d for c, d in zip(a, b)), data))
                    self.data_disks[failed_disk_idx].write(parity)
                else:
                    data = []
                    parity = self.data_disks[parity_idx].read()
                    for i in range(self.num_disks - 1):
                        if i != parity_idx:
                            disk_data = self.data_disks[i].read()
                            data.append(disk_data)
                            if i == failed_disk_idx:
                                self.data_disks[i].write(bytes(a ^ b for a, b in zip(parity, reduce(lambda c, d: bytes(e ^ f for e, f in zip(c, d)), data))))
# Create a RAID 0 array with 4 disks, block size 512 bytes
raid0 = RAIDArray(block_size=512, num_disks=4, raid_level=0)

# Write data to RAID 0
data = b'Hello, World!' * 1000
raid0.write(0, data)

# Read data from RAID 0
read_data = raid0.read(0)
print(f"RAID 0 read data: {read_data}")
# RAID 0 read data: b'Hello, World!Hello, World!Hello, World!...'

# Create a RAID 1 array with 4 disks, block size 512 bytes
raid1 = RAIDArray(block_size=512, num_disks=4, raid_level=1, num_spare=1)

# Write data to RAID 1
data = b'Goodbye!' * 500
raid1.write(0, data)

# Read data from RAID 1
read_data = raid1.read(0)
print(f"RAID 1 read data: {read_data}")
# RAID 1 read data: b'Goodbye!Goodbye!Goodbye!Goodbye!...'

# Simulate disk failure and rebuild for RAID 1
raid1.rebuild(1)
read_data = raid1.read(0)
print(f"RAID 1 read data after rebuild: {read_data}")
```
In the above implementation:
- We create a RAIDArray instance with 4 disks, block size 512 bytes, and RAID level 0 (striping).
- We write data to the RAID 0 array and read it back.
- We create another RAIDArray instance with 4 disks, block size 512 bytes, RAID level 1 (mirroring), and 1 spare disk.
- We write data to the RAID 1 array, read it back, simulate a disk failure, and rebuild the array using the spare disk.

## Task 3: Buffer pool management
A database buffer pool is a cache in a database management system (DBMS) that stores database pages in memory, allowing quicker access to frequently accessed data. 

```javascript
class BufferPool:
    def __init__(self, pool_size):
        self.pool_size = pool_size
        self.buffer = {}
        self.usage = {}  # To track usage frequency

    def fetch_page(self, page_id):
        if page_id in self.buffer:
            # If page already in buffer, return it and update usage
            self.usage[page_id] += 1
            return self.buffer[page_id]
        else:
            # If page not in buffer, fetch from disk (not implemented)
            # For demonstration, we're just generating a dummy page
            page_content = f"Page {page_id} content"
            self.usage[page_id] = 1
            if len(self.buffer) < self.pool_size:
                # If buffer not full, add page to buffer
                self.buffer[page_id] = page_content
            else:
                # If buffer full, evict least recently used page
                lru_page = min(self.usage, key=self.usage.get)
                del self.buffer[lru_page]
                del self.usage[lru_page]
                self.buffer[page_id] = page_content
            return page_content

# Example usage
buffer_pool = BufferPool(3)  # Initialize buffer pool with size 3

# Fetch pages
print(buffer_pool.fetch_page(1))  # Page 1 content
print(buffer_pool.fetch_page(2))  # Page 2 content
print(buffer_pool.fetch_page(3))  # Page 3 content

# Accessing existing pages again should not evict any page
print(buffer_pool.fetch_page(1))  # Page 1 content
print(buffer_pool.fetch_page(2))  # Page 2 content

# Fetching a new page should evict the least recently used page (Page 3)
print(buffer_pool.fetch_page(4))  # Page 4 content
```

- BufferPool class represents the buffer pool with methods to fetch pages.
- __init__ method initializes the buffer pool with a specified size.
- fetch_page method is responsible for fetching a page from the buffer pool. If the page is already in the buffer, it updates its usage count.
- If not, it fetches the page from disk (not implemented in this example) and adds it to the buffer pool. If the buffer is full, it evicts the least recently used page.

# Exercise 

- Read relevant materials and outline the important procedures to be followed when building a relational database from scractch.
  - List data structures to be used
  - Explain the importance of each outlined procedure

Example References: 
- [chidb:Building a Simple Relational Database System from Scratch](https://people.cs.uchicago.edu/~adamshaw/papers/sigcse2016-chidb.pdf)
- [How to build a relational database from scratch](https://medium.com/swlh/how-to-build-a-relational-database-from-scratch-e208061027c7)

